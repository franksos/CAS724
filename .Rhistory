library(devtools)
install_github("mukul13/rword2vec")
library(devtools)
install_github("mukul13/rword2vec")
train <- read.csv("CoNLL2003/eng.train", sep = " ", stringsAsFactors = F)
View(train)
setwd("~/Documents/McMaster/CAS764/Project/")
library(dplyr)
library(rJava)
library(NLP)
library(keras)
train <- read.csv("CoNLL2003/eng.train", sep = " ", stringsAsFactors = F)
testa <- read.csv("CoNLL2003/eng.testa", sep = " ", stringsAsFactors = F)
testb <- read.csv("CoNLL2003/eng.testb", sep = " ", stringsAsFactors = F)
testc <- read.csv("CoNLL2003/eng.testc", sep = " ", stringsAsFactors = F)
testc <- read.csv("CoNLL2003/eng.testc", sep = " ", header = F, stringsAsFactors = F)
names(train)<- c("word","POS","chunk","entity")
names(testa)<- c("word","POS","chunk","entity")
names(testb)<- c("word","POS","chunk","entity")
names(testc)<- c("word","POS","chunk","entity")
train <- mutate(train, ds = "train")
testa <- mutate(testa, ds = "testa")
testb <- mutate(testb, ds = "testb")
testc <- mutate(testc, ds = "testc")
ds <- rbind(train, testa, testb, testc)
unique(ds$entity)
ds$entity[ds$entity == "I-ORG" | ds$entity == "B-ORG"] <- "ORG"
ds$entity[ds$entity == "I-LOC" | ds$entity == "B-LOC"] <- "LOC"
ds$entity[ds$entity == "I-PER" | ds$entity == "B-PER"] <- "PER"
ds$entity[ds$entity == "I-MISC" | ds$entity == "B-MISC"] <- "MISC"
ds$entity[ds$entity == "O" | ds$entity == ""] <- "NE"
unique(ds$entity)
ds$entity <- as.factor(ds$entity)
uniqword <- length(unique(ds$word))
tok <- text_tokenizer(num_words = uniqword) %>% fit_text_tokenizer(ds$word)
## this causes error
## t_x <- texts_to_matrix(tok, ds$word, mode = 'tfidf')
View(train)
library(keras)
version()
demo()
library(dplyr)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
library(keras)
# Data Preparation -----------------------------------------------------
batch_size <- 128
num_classes <- 10
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define Model -----------------------------------------------------------
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
length(c(3,4))
for (i in 1 : prev) {
xt_prev[i] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_rwdt[i]) <- c(d[2],d[1])
}
x <- c(1,2,4,5,7,8,1,3,4,6,7,9,1,2,4,5,7,8,1,3,4,6,7,9)
dim(x) <- c(6,4)
prev <- 3
nxt <- 4
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[i] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[i]) <- c(d[2],d[1])
x <- c(t(xt_prev[i]),x)
}
for (i in 1 : nxt) {
xt_nxt[i] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[i]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[i]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt_nxt[1]
xt_nxt[[1]]
xt_nxt[[2]]
xt_nxt[2]
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]))
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
x <- c(1,2,4,5,7,8,1,3,4,6,7,9,1,2,4,5,7,8,1,3,4,6,7,9)
dim(x) <- c(6,4)
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
x
x[1, ,,]
x[6,,,]
x[4,,,]
library(depmixS4)
install.packages('demmixS4')
install.packages('depmixS4')
#----------------------------------------------------------------
# Hiden Markov Model of S&P 500 log returns
# See documentation for depmixS4 package
# http://cran.r-project.org/web/packages/depmixS4/depmixS4.pdf and presentation
# on Singapore R Users Group Site on HMM February 14, 2014
# http://www.meetup.com/R-User-Group-SG/files/
library(depmixS4)
library(TTR)
library(ggplot2)
library(reshape2)
## Bull and Bear Markets ##
# Load S&P 500 returns from Yahoo
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
head(sp500)
tail(sp500)
# Preprocessing
# Compute log Returns
ep <- endpoints(sp500, on = "months", k = 1)
sp500LR <- sp500[ep[2:(length(ep)-1)]]
sp500LR$logret <- log(sp500LR$Close) - lag(log(sp500LR$Close))
sp500LR <- na.exclude(sp500LR)
head(sp500LR)
# Build a data frame for ggplot
sp500LRdf <- data.frame(sp500LR)
sp500LRdf$Date <-as.Date(row.names(sp500LRdf),"%Y-%m-%d")
# Plot the S&P 500 returns
ggplot( sp500LRdf, aes(Date) ) +
geom_line( aes( y = logret ) ) +
labs( title = "S&P 500 log Returns")
# Construct and fit a regime switching model
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500LR)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
#
summary(fm2)
print(fm2)
# Classification (inference task)
probs <- posterior(fm2)             # Compute probability of being in each state
head(probs)
rowSums(head(probs)[,2:5])          # Check that probabilities sum to 1
pBear <- probs[,2]                  # Pick out the "Bear" or low volatility state
sp500LRdf$pBear <- pBear            # Put pBear in the data frame for plotting
# Pick out an interesting subset of the data or plotting and
# reshape the data in a form convenient for ggplot
df <- melt(sp500LRdf[400:500,6:8],id="Date",measure=c("logret","pBear"))
#head(df)
# Plot the log return time series along withe the time series of probabilities
qplot(Date,value,data=df,geom="line",
main = "SP 500 Log returns and 'Bear' state probabilities",
ylab = "") +
facet_grid(variable ~ ., scales="free_y")
quantmod::getSymbols
sp500 <- quantmod::getSymbols("^GSPC", start = 19500101, end = 20120909, freq = "daily")
sp500
?getSymbols.yahoo
sp500 <- quantmod::getSymbols.yahoo("^GSPC", from = "1950-01-01", to = "2012-09-09", periodicity = "daily")
sp500 <- quantmod::getSymbols.yahoo("^GSPC", env=globalenv(), from = "1950-01-01", to = "2012-09-09", periodicity = "daily")
sp500
sp500 <- GSPC
head(sp500)
tail(sp500)
# Preprocessing
# Compute log Returns
ep <- endpoints(sp500, on = "months", k = 1)
sp500LR <- sp500[ep[2:(length(ep)-1)]]
sp500LR$logret <- log(sp500LR$Close) - lag(log(sp500LR$Close))
sp500LR <- na.exclude(sp500LR)
head(sp500LR)
# Build a data frame for ggplot
sp500LRdf <- data.frame(sp500LR)
sp500LRdf$Date <-as.Date(row.names(sp500LRdf),"%Y-%m-%d")
# Plot the S&P 500 returns
ggplot( sp500LRdf, aes(Date) ) +
geom_line( aes( y = logret ) ) +
labs( title = "S&P 500 log Returns")
# Construct and fit a regime switching model
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500LR)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
#
summary(fm2)
print(fm2)
# Classification (inference task)
probs <- posterior(fm2)             # Compute probability of being in each state
head(probs)
rowSums(head(probs)[,2:5])          # Check that probabilities sum to 1
pBear <- probs[,2]                  # Pick out the "Bear" or low volatility state
sp500LRdf$pBear <- pBear            # Put pBear in the data frame for plotting
# Pick out an interesting subset of the data or plotting and
# reshape the data in a form convenient for ggplot
df <- melt(sp500LRdf[400:500,6:8],id="Date",measure=c("logret","pBear"))
#head(df)
# Plot the log return time series along withe the time series of probabilities
qplot(Date,value,data=df,geom="line",
main = "SP 500 Log returns and 'Bear' state probabilities",
ylab = "") +
facet_grid(variable ~ ., scales="free_y")
library(seqHMM)
data(biofam3c)
install.packages('seqhmm')
install.packages('seqHMM')
data(biofam3c)
?biofam3c
??biofam3c
install.packages('TraMineR')
library(TraMineR)
??biofam3c
data(biofam3c)
install.packages("acepack")
exit()
library("seqHMM")
install.packages("seqHMM")
install.packages("seqHMM")
install.packages("seqHMM")
library(seqHMM)
# Reset the workplace
rm(list=ls())
setwd("~/Documents/McMaster/CAS764/Project/")
library(keras)
library(caret)
# a customized function to change probilities to classes
prob_to_class <- function (p) {
k <- 0
for(i in 1:dim(p)[1]) {
k[i] <- which(p[i,] == max(p[i,]))
}
return (k - 1) # Keras classfication starts from 0 and R from 1
}
# Data Preparation from pre-processed
x_train <- readRDS("NEEL2016/NEELx_train.rds")
x_dev <- readRDS("NEEL2016/NEELx_dev.rds")
x_test <- readRDS("NEEL2016/NEELx_test.rds")
y_train <- readRDS("NEEL2016/NEELy_train.rds")
y_dev <- readRDS("NEEL2016/NEELy_dev.rds")
y_test <- readRDS("NEEL2016/NEELy_test.rds")
y_truth <-readRDS("NEEL2016/NEELy_Truth.rds")
# Below model is from MNIST RNN example
set.seed(116)  # for reproductibility
# Training parameters.
batch_size <- 32
num_classes <- 8 # 7 levels of entities 1:7 but python starts from 0
epochs <- 10
# Embedding dimensions.
row_hidden <- 128
col_hidden <- 128
dim_x_train <- dim(x_train)
cat('x_train_shape:', dim_x_train)
cat(nrow(x_train), 'train samples')
cat(nrow(x_test), 'test samples')
# Define input dimensions
row <- dim_x_train[[2]]
col <- dim_x_train[[3]]
pixel <- dim_x_train[[4]]
# Model input (4D)
input <- layer_input(shape = c(row, col, pixel))
# Encodes a row of pixels using TimeDistributed Wrapper
encoded_rows <- input %>% time_distributed(layer_lstm(units = row_hidden))
# Encodes columns of encoded rows
encoded_columns <- encoded_rows %>% layer_lstm(units = col_hidden)
# Model output
prediction <- encoded_columns %>%
layer_dense(units = num_classes, activation = 'softmax')
# Define Model
model <- keras_model(input, prediction)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'rmsprop',
metrics = c('accuracy')
)
# Training model and measure running time
start_time <- Sys.time()
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_data = list(x_dev, y_dev)
)
end_time <- Sys.time()
# Evaluation
scores <- model %>% evaluate(x_test, y_test, verbose = 0)
# Output metrics
start_time
end_time
# Report of three test groups
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')
# Further reports - report precision, recall and F1 score
y_prob <- model %>% predict(x_test)
y_pred <- prob_to_class(y_prob)
entitylevel <- c("Character","Event","Location",
"Organization", "Person","Product","Thing")
y_pred <- factor(y_pred, levels = c(1:7))
levels(y_pred) <- entitylevel
mx <- confusionMatrix(y_pred, y_truth)
cat('Test set A details: \n')
mx
mx[4]
