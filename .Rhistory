library(devtools)
install_github("mukul13/rword2vec")
train <- read.csv("CoNLL2003/eng.train", sep = " ", stringsAsFactors = F)
View(train)
setwd("~/Documents/McMaster/CAS764/Project/")
library(dplyr)
library(rJava)
library(NLP)
library(keras)
train <- read.csv("CoNLL2003/eng.train", sep = " ", stringsAsFactors = F)
testa <- read.csv("CoNLL2003/eng.testa", sep = " ", stringsAsFactors = F)
testb <- read.csv("CoNLL2003/eng.testb", sep = " ", stringsAsFactors = F)
testc <- read.csv("CoNLL2003/eng.testc", sep = " ", stringsAsFactors = F)
testc <- read.csv("CoNLL2003/eng.testc", sep = " ", header = F, stringsAsFactors = F)
names(train)<- c("word","POS","chunk","entity")
names(testa)<- c("word","POS","chunk","entity")
names(testb)<- c("word","POS","chunk","entity")
names(testc)<- c("word","POS","chunk","entity")
train <- mutate(train, ds = "train")
testa <- mutate(testa, ds = "testa")
testb <- mutate(testb, ds = "testb")
testc <- mutate(testc, ds = "testc")
ds <- rbind(train, testa, testb, testc)
unique(ds$entity)
ds$entity[ds$entity == "I-ORG" | ds$entity == "B-ORG"] <- "ORG"
ds$entity[ds$entity == "I-LOC" | ds$entity == "B-LOC"] <- "LOC"
ds$entity[ds$entity == "I-PER" | ds$entity == "B-PER"] <- "PER"
ds$entity[ds$entity == "I-MISC" | ds$entity == "B-MISC"] <- "MISC"
ds$entity[ds$entity == "O" | ds$entity == ""] <- "NE"
unique(ds$entity)
ds$entity <- as.factor(ds$entity)
uniqword <- length(unique(ds$word))
tok <- text_tokenizer(num_words = uniqword) %>% fit_text_tokenizer(ds$word)
## this causes error
## t_x <- texts_to_matrix(tok, ds$word, mode = 'tfidf')
View(train)
library(keras)
version()
demo()
library(dplyr)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
library(keras)
# Data Preparation -----------------------------------------------------
batch_size <- 128
num_classes <- 10
epochs <- 12
# Input image dimensions
img_rows <- 28
img_cols <- 28
# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define Model -----------------------------------------------------------
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
length(c(3,4))
for (i in 1 : prev) {
xt_prev[i] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_rwdt[i]) <- c(d[2],d[1])
}
x <- c(1,2,4,5,7,8,1,3,4,6,7,9,1,2,4,5,7,8,1,3,4,6,7,9)
dim(x) <- c(6,4)
prev <- 3
nxt <- 4
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[i] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[i]) <- c(d[2],d[1])
x <- c(t(xt_prev[i]),x)
}
for (i in 1 : nxt) {
xt_nxt[i] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[i]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[i]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt_nxt[1]
xt_nxt[[1]]
xt_nxt[[2]]
xt_nxt[2]
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]))
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
x <- c(1,2,4,5,7,8,1,3,4,6,7,9,1,2,4,5,7,8,1,3,4,6,7,9)
dim(x) <- c(6,4)
d <- dim(x)
if(length(d) != 2) stop("input not 2D with tuple and attribute")
xt_prev <- vector("list", prev)
xt_nxt <- vector("list", nxt)
xt <- as.vector(t(x))
for (i in 1 : prev) {
xt_prev[[i]] <- c(rep(0,d[2]*i), xt[1:(length(xt)-d[2]*i)])
dim(xt_prev[[i]]) <- c(d[2],d[1])
x <- c(t(xt_prev[[i]]),x)
}
for (i in 1 : nxt) {
xt_nxt[[i]] <- c(xt[(d[2]*i + 1):(length(xt))],rep(0, d[2]*i))
dim(xt_nxt[[i]]) <- c(d[2],d[1])
x <- c(x, t(xt_nxt[[i]]))
}
dim(x) <- c(d[1],d[2],(prev+nxt+1),1)
return(x)
x
x[1, ,,]
x[6,,,]
x[4,,,]
library(depmixS4)
install.packages('demmixS4')
install.packages('depmixS4')
#----------------------------------------------------------------
# Hiden Markov Model of S&P 500 log returns
# See documentation for depmixS4 package
# http://cran.r-project.org/web/packages/depmixS4/depmixS4.pdf and presentation
# on Singapore R Users Group Site on HMM February 14, 2014
# http://www.meetup.com/R-User-Group-SG/files/
library(depmixS4)
library(TTR)
library(ggplot2)
library(reshape2)
## Bull and Bear Markets ##
# Load S&P 500 returns from Yahoo
Sys.setenv(tz = "UTC")
sp500 <- getYahooData("^GSPC", start = 19500101, end = 20120909, freq = "daily")
head(sp500)
tail(sp500)
# Preprocessing
# Compute log Returns
ep <- endpoints(sp500, on = "months", k = 1)
sp500LR <- sp500[ep[2:(length(ep)-1)]]
sp500LR$logret <- log(sp500LR$Close) - lag(log(sp500LR$Close))
sp500LR <- na.exclude(sp500LR)
head(sp500LR)
# Build a data frame for ggplot
sp500LRdf <- data.frame(sp500LR)
sp500LRdf$Date <-as.Date(row.names(sp500LRdf),"%Y-%m-%d")
# Plot the S&P 500 returns
ggplot( sp500LRdf, aes(Date) ) +
geom_line( aes( y = logret ) ) +
labs( title = "S&P 500 log Returns")
# Construct and fit a regime switching model
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500LR)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
#
summary(fm2)
print(fm2)
# Classification (inference task)
probs <- posterior(fm2)             # Compute probability of being in each state
head(probs)
rowSums(head(probs)[,2:5])          # Check that probabilities sum to 1
pBear <- probs[,2]                  # Pick out the "Bear" or low volatility state
sp500LRdf$pBear <- pBear            # Put pBear in the data frame for plotting
# Pick out an interesting subset of the data or plotting and
# reshape the data in a form convenient for ggplot
df <- melt(sp500LRdf[400:500,6:8],id="Date",measure=c("logret","pBear"))
#head(df)
# Plot the log return time series along withe the time series of probabilities
qplot(Date,value,data=df,geom="line",
main = "SP 500 Log returns and 'Bear' state probabilities",
ylab = "") +
facet_grid(variable ~ ., scales="free_y")
quantmod::getSymbols
sp500 <- quantmod::getSymbols("^GSPC", start = 19500101, end = 20120909, freq = "daily")
sp500
?getSymbols.yahoo
sp500 <- quantmod::getSymbols.yahoo("^GSPC", from = "1950-01-01", to = "2012-09-09", periodicity = "daily")
sp500 <- quantmod::getSymbols.yahoo("^GSPC", env=globalenv(), from = "1950-01-01", to = "2012-09-09", periodicity = "daily")
sp500
sp500 <- GSPC
head(sp500)
tail(sp500)
# Preprocessing
# Compute log Returns
ep <- endpoints(sp500, on = "months", k = 1)
sp500LR <- sp500[ep[2:(length(ep)-1)]]
sp500LR$logret <- log(sp500LR$Close) - lag(log(sp500LR$Close))
sp500LR <- na.exclude(sp500LR)
head(sp500LR)
# Build a data frame for ggplot
sp500LRdf <- data.frame(sp500LR)
sp500LRdf$Date <-as.Date(row.names(sp500LRdf),"%Y-%m-%d")
# Plot the S&P 500 returns
ggplot( sp500LRdf, aes(Date) ) +
geom_line( aes( y = logret ) ) +
labs( title = "S&P 500 log Returns")
# Construct and fit a regime switching model
mod <- depmix(logret ~ 1, family = gaussian(), nstates = 4, data = sp500LR)
set.seed(1)
fm2 <- fit(mod, verbose = FALSE)
#
summary(fm2)
print(fm2)
# Classification (inference task)
probs <- posterior(fm2)             # Compute probability of being in each state
head(probs)
rowSums(head(probs)[,2:5])          # Check that probabilities sum to 1
pBear <- probs[,2]                  # Pick out the "Bear" or low volatility state
sp500LRdf$pBear <- pBear            # Put pBear in the data frame for plotting
# Pick out an interesting subset of the data or plotting and
# reshape the data in a form convenient for ggplot
df <- melt(sp500LRdf[400:500,6:8],id="Date",measure=c("logret","pBear"))
#head(df)
# Plot the log return time series along withe the time series of probabilities
qplot(Date,value,data=df,geom="line",
main = "SP 500 Log returns and 'Bear' state probabilities",
ylab = "") +
facet_grid(variable ~ ., scales="free_y")
library(seqHMM)
data(biofam3c)
install.packages('seqhmm')
install.packages('seqHMM')
data(biofam3c)
?biofam3c
??biofam3c
install.packages('TraMineR')
library(TraMineR)
??biofam3c
data(biofam3c)
install.packages("acepack")
exit()
library("seqHMM")
install.packages("seqHMM")
install.packages("seqHMM")
install.packages("seqHMM")
library(seqHMM)
library(TraMineR)
data("biofam")
head(biofam, [,10:25])
head(biofam[,10:25])
biofam.seq <- seqdef(biofam[,10:25, start=15,labels = c("parent","left","married","left+marr","child", "left+cild", "left+marr+child","divorced")])
biofam.seq <- seqdef(biofam[,10:25], start=15,labels = c("parent","left","married","left+marr","child", "left+cild", "left+marr+child","divorced"))
data(biofam3c)
marr.seq <- seqdef(biofam3c$married, start = 15, alphabet = c("single", "married", "divorced"))
child.seq <- seqdef(biofam3c$children, start = 15, alphabet = c("childless", "children"))
left.seq <- seqdef(biofam3c$left, start = 15, alphabet = c("with parents", "left home"))
ssplot(
list("Marriage" = marr.seq, "Parenthood" = child.seq,
"Residence" = left.seq))
ssp2 <- ssp(
list(marr.seq, child.seq, left.seq),
type = "I",title = "Sequence index plots",
sortv = "from.start", sort.channel = 1,
withlegend = FALSE, ylab.pos = c(1, 1.5, 1),
ylab = c("Marriage", "Parenthood", "Residence"))
plot(ssp2)
ssp2 <- ssp(
list(marr.seq, child.seq, left.seq),
type = "I",title = "Sequence index plots",
sortv = "from.start", sort.channel = 1,
with.legend = FALSE, ylab.pos = c(1, 1.5, 1),
ylab = c("Marriage", "Parenthood", "Residence"))
plot(ssp2)
?sink()
outa <- read.csv("HMM/outputa",sep = " ", stringsAsFactors = F)
system("python HMM/HMM.py HMM/eng.train HMM/eng.testa outputa")
setwd("~/Documents/McMaster/CAS764/Project/HMM")
sink(file = 'Reports/CONLLHMM raw R result.txt')
system("python HMM.py eng.train eng.testa outputa")
# This algorithm runs an HMM prediction model for the CONLL dataset
# the HMM.py code credit goes to tripleday:
# https://github.com/tripleday/simple_HMM
# The three test datasets were already individually evaluated in the CNN/RNN algorithm
# and in this HMM algorithm I combine the test files together to evaluate
setwd("~/Documents/McMaster/CAS764/Project/HMM")
sink(file = '../Reports/CONLLHMM raw R result.txt')
system("python HMM.py eng.train eng.testa outputa")
system("python HMM.py eng.train eng.testb outputb")
system("python HMM.py eng.train eng.testc outputc")
setwd("~/Documents/McMaster/CAS764/Project")
library(caret)
outa <- read.csv("HMM/outputa",sep = " ", stringsAsFactors = F)
outb <- read.csv("HMM/outputb",sep = " ", stringsAsFactors = F)
outc <- read.csv("HMM/outputc",sep = " ", stringsAsFactors = F)
#output column 4 is the truth value and column 5 is the predicted value
names(outa)<- c("word","POS","chunk","ent_t", "ent_p")
names(outb)<- c("word","POS","chunk","ent_t", "ent_p")
names(outc)<- c("word","POS","chunk","ent_t", "ent_p")
clean_entity <- function(char) {
char[char == "I-ORG" | char == "B-ORG"] <- "ORG"
char[char == "I-LOC" | char == "B-LOC"] <- "LOC"
char[char == "I-PER" | char == "B-PER"] <- "PER"
char[char == "I-MISC" | char == "B-MISC"] <- "MISC"
char[char == "O" | char == ""] <- "O"
return(char)
}
outa_truth <- clean_entity(outa$ent_t)
outa_pred <- clean_entity(outa$ent_p)
outb_truth <- clean_entity(outb$ent_t)
outb_pred <- clean_entity(outb$ent_p)
outc_truth <- clean_entity(outca$ent_t)
outc_pred <- clean_entity(outc$ent_p)
entitylevel <- c("O", "LOC","MISC","ORG","PER")
outa_truth <- factor(outa_truth, levels = entitylevel)
outa_pred <- factor(outa_pred, levels = entitylevel)
outb_truth <- factor(outb_truth, levels = entitylevel)
outb_pred <- factor(outb_pred, levels = entitylevel)
outc_truth <- factor(outc_truth, levels = entitylevel)
outc_pred <- factor(outc_pred, levels = entitylevel)
cat('test a \n')
mx_a <- confusionMatrix(outa_pred, outa_truth)
mx_a
mx_a[4]
cat('test b \n')
mx_b <- confusionMatrix(outb_pred, outb_truth)
mx_b
mx_b[4]
cat('test c \n')
mx_c <- confusionMatrix(outc_pred, outc_truth)
mx_c
mx_c[4]
sink()
# This algorithm runs an HMM prediction model for the CONLL dataset
# the HMM.py code credit goes to tripleday:
# https://github.com/tripleday/simple_HMM
# The three test datasets were already individually evaluated in the CNN/RNN algorithm
# and in this HMM algorithm I combine the test files together to evaluate
setwd("~/Documents/McMaster/CAS764/Project/HMM")
sink(file = '../Reports/CONLLHMM raw R result.txt')
starttime <- system.time()
system("python HMM.py eng.train eng.testa outputa")
system("python HMM.py eng.train eng.testb outputb")
system("python HMM.py eng.train eng.testc outputc")
endtime <- system.time()
setwd("~/Documents/McMaster/CAS764/Project")
library(caret)
outa <- read.csv("HMM/outputa",sep = " ", stringsAsFactors = F)
outb <- read.csv("HMM/outputb",sep = " ", stringsAsFactors = F)
outc <- read.csv("HMM/outputc",sep = " ", stringsAsFactors = F)
#output column 4 is the truth value and column 5 is the predicted value
names(outa)<- c("word","POS","chunk","ent_t", "ent_p")
names(outb)<- c("word","POS","chunk","ent_t", "ent_p")
names(outc)<- c("word","POS","chunk","ent_t", "ent_p")
clean_entity <- function(char) {
char[char == "I-ORG" | char == "B-ORG"] <- "ORG"
char[char == "I-LOC" | char == "B-LOC"] <- "LOC"
char[char == "I-PER" | char == "B-PER"] <- "PER"
char[char == "I-MISC" | char == "B-MISC"] <- "MISC"
char[char == "O" | char == ""] <- "O"
return(char)
}
outa_truth <- clean_entity(outa$ent_t)
outa_pred <- clean_entity(outa$ent_p)
outb_truth <- clean_entity(outb$ent_t)
outb_pred <- clean_entity(outb$ent_p)
outc_truth <- clean_entity(outc$ent_t)
outc_pred <- clean_entity(outc$ent_p)
entitylevel <- c("O", "LOC","MISC","ORG","PER")
outa_truth <- factor(outa_truth, levels = entitylevel)
outa_pred <- factor(outa_pred, levels = entitylevel)
outb_truth <- factor(outb_truth, levels = entitylevel)
outb_pred <- factor(outb_pred, levels = entitylevel)
outc_truth <- factor(outc_truth, levels = entitylevel)
outc_pred <- factor(outc_pred, levels = entitylevel)
print(starttime)
print(endtime)
cat('test a \n')
mx_a <- confusionMatrix(outa_pred, outa_truth)
mx_a
mx_a[4]
cat('test b \n')
mx_b <- confusionMatrix(outb_pred, outb_truth)
mx_b
mx_b[4]
cat('test c \n')
mx_c <- confusionMatrix(outc_pred, outc_truth)
mx_c
mx_c[4]
sink()
starttime <- system.time()
# This algorithm runs an HMM prediction model for the CONLL dataset
# the HMM.py code credit goes to tripleday:
# https://github.com/tripleday/simple_HMM
# The three test datasets were already individually evaluated in the CNN/RNN algorithm
# and in this HMM algorithm I combine the test files together to evaluate
setwd("~/Documents/McMaster/CAS764/Project/HMM")
sink(file = '../Reports/CONLLHMM raw R result.txt')
starttime <- Sys.time()
system("python HMM.py eng.train eng.testa outputa")
system("python HMM.py eng.train eng.testb outputb")
system("python HMM.py eng.train eng.testc outputc")
endtime <- Sys.time()
setwd("~/Documents/McMaster/CAS764/Project")
library(caret)
outa <- read.csv("HMM/outputa",sep = " ", stringsAsFactors = F)
outb <- read.csv("HMM/outputb",sep = " ", stringsAsFactors = F)
outc <- read.csv("HMM/outputc",sep = " ", stringsAsFactors = F)
#output column 4 is the truth value and column 5 is the predicted value
names(outa)<- c("word","POS","chunk","ent_t", "ent_p")
names(outb)<- c("word","POS","chunk","ent_t", "ent_p")
names(outc)<- c("word","POS","chunk","ent_t", "ent_p")
clean_entity <- function(char) {
char[char == "I-ORG" | char == "B-ORG"] <- "ORG"
char[char == "I-LOC" | char == "B-LOC"] <- "LOC"
char[char == "I-PER" | char == "B-PER"] <- "PER"
char[char == "I-MISC" | char == "B-MISC"] <- "MISC"
char[char == "O" | char == ""] <- "O"
return(char)
}
outa_truth <- clean_entity(outa$ent_t)
outa_pred <- clean_entity(outa$ent_p)
outb_truth <- clean_entity(outb$ent_t)
outb_pred <- clean_entity(outb$ent_p)
outc_truth <- clean_entity(outc$ent_t)
outc_pred <- clean_entity(outc$ent_p)
entitylevel <- c("O", "LOC","MISC","ORG","PER")
outa_truth <- factor(outa_truth, levels = entitylevel)
outa_pred <- factor(outa_pred, levels = entitylevel)
outb_truth <- factor(outb_truth, levels = entitylevel)
outb_pred <- factor(outb_pred, levels = entitylevel)
outc_truth <- factor(outc_truth, levels = entitylevel)
outc_pred <- factor(outc_pred, levels = entitylevel)
print(starttime)
print(endtime)
cat('test a \n')
mx_a <- confusionMatrix(outa_pred, outa_truth)
mx_a
mx_a[4]
cat('test b \n')
mx_b <- confusionMatrix(outb_pred, outb_truth)
mx_b
mx_b[4]
cat('test c \n')
mx_c <- confusionMatrix(outc_pred, outc_truth)
mx_c
mx_c[4]
sink()
